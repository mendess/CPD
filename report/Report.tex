\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[cache=false]{minted}
\usepackage[dvipsnames]{xcolor}
\usepackage{a4wide,syntax,listings,appendix,tikz,wrapfig,graphicx,hyperref}
\hypersetup{pdftitle={g08report},
pdfauthor={Pedro Mendes},
colorlinks=true,
urlcolor=blue,
linkcolor=black}
%\usetikzlibrary{arrows,positioning,automata,decorations.markings,shadows,shapes,calc}

\begin{document}

\title{Recomender System}
\author{Pedro Mendes (97144), Filipe Lucas (78775), Ricardo Pereira (86506)}
\date{\today}
\maketitle
%\tableofcontents

\section{Data Structure}
There was multiple approaches to store the matrix A, the most basic one was storing the values in a adjacency matrix however as the matrix A is a sparse one we opted to store it in a compressed sparse row (CSR). The CSR allows to store just the non-zero values in a single array making this a good choice in terms of caching since all the values are stored in adjancent memory positions

\section{Matrix B multiplication}
The new matrix B is calculated in every iteration however since the only positions that change are the ones who are different from zero in Matrix A we only multiply specific positions instead of the full matrix.
\section{Serial Vers}

\section{Problem}

\section{Solution}

%very wip
%TODO divide into smaller sections

%the approach used for parallelization
%what decomposition was used
%what were the synchronization concerns and why
%how was load balancing addressed -> iter_l chunks?
%what are the performance results, and are they what you expected

%im noob pls add paragraph here 

The approach we took for parallelization was quite methodic. First was to optimize as 
far as we could the sequential version. We believed that the optimizations could help the
parallelization process. Second was to identify CPU-heavy tasks that could be  
parallelizable. Considering this, the sections identified were the calculations of the three 
matrices: B, L and R.  We started by parallelizing the calculation of matrix B, as it is
considerably easier than the others. This parallelization could be easily done because
there's no data dependencies within the calculation. Considering this, the parallelization
was done on the outer for loop of the matrix calculation, where the rows are distribitued 
for the threads. After dealing with the easilly parallelizable sections, we started to take a look
into the code for calculating the L and R matrices. Due to the optimizations performed in
the sequential version, there was no race conditions in these calculations. %explain!!!
To parallelize the calculation of the R matrix, the same tecnhique used in the matrix B 
calculation was used: parallelize the outer loop. In all cases, trying to parallelize
the most inner loop only brought a time loss when comparing to the outer loop parallelization.
This was most likely caused by the overhead of calling threads more frequently. 
%TODO parallelization of L calc
For both calculations of matrix L and R, the decomposition was the same way it was done
for matrix B's calculation, by distributing the rows for the threads.
for the threads.

%...

As for the scheduling, using dynamic brought a better speedup in some cases, but also made
some instances slower. While using static scheduling, the slower instances have a common 
characteristic: the A matrix has some rows which have substantially more elements than others. 
This translated in a bad load balacing, as some threads will have to wait for the one dealing
with the bigger rows. 
%Taking this in consideration, the decision to keep a static schedule
%was made. keeping here not to forget to add better conclusion


%...

It is also important to note that, in bigger instances, the parsing creates a considerable 
overhead, making the speedup significantly worse.

%still rough
After the optimizations, the performance for the sequential version fitted within 
expectations. This can be attributed to the numerous optimizations and data structures used.
As for the parallel version, the speedups were quite good taking into account how we decided 
to tackle the parallelization. In the smaller instances the speedup was worse, as expected,
mainly because of the overhead introduced by the parallelization process. Excluding the 
instances where the parsing creates an overhead, the speedups were quite good, getting between X %TODO 
and 3.5.


%ADD PRETTY GRAPHS!
%add tables
\end{document}

