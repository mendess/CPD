\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[cache=false]{minted}
\usepackage[dvipsnames]{xcolor}
\usepackage{a4wide,syntax,listings,appendix,tikz,wrapfig,graphicx,hyperref}
\hypersetup{pdftitle={g08report},
pdfauthor={Pedro Mendes},
colorlinks=true,
urlcolor=blue,
linkcolor=black}
%\usetikzlibrary{arrows,positioning,automata,decorations.markings,shadows,shapes,calc}

\begin{document}

\title{Recomender System}
\author{Pedro Mendes (97144), Filipe Lucas (78775), Ricardo Pereira (86506)}
\date{\today}
\maketitle
%\tableofcontents

\section{Data Structure}
There was multiple approaches to store the matrix A, the most basic one was storing the values in a adjacency matrix however as the matrix A is a sparse one we opted to store it in a compressed sparse row (CSR). The CSR allows to store just the non-zero values in a single array making this a good choice in terms of caching since all the values are stored in adjancent memory positions

\section{Matrix B multiplication}
The new matrix B is calculated in every iteration however since the only positions that change are the ones who are different from zero in Matrix A we only multiply specific positions instead of the full matrix.
\section{Serial Vers}

\section{Problem}

\section{Solution}

%very wip
%TODO divide into smaller sections

%the approach used for parallelization
%what decomposition was used -> TODO
%what were the synchronization concerns and why
%how was load balancing addressed -> iter_l chunks
%what are the performance results, and are they what you expected

%im noob pls add paragraph here 

The approach we took for parallelization was quite methodic. First was to optimize as 
far as we could the sequential version. We believed that the optimizations could help the
parallelization process. Second was to look for easily parallelizable sections of the code.
The first we identified was the computation of matrix B that, in our case, is divided into
two functions due to optimization concerns. %expand this?  
This parallelization could be easily done because there's no data dependencies within the 
calculation. Considering this, the parallelization was done on the outer for loop of the 
matrix calculation, where the rows are distribitued for the threads.

After dealing with the easilly parallelizable sections, we started to take a look
into the code for calculating the L and R matrices. Due to the optimizations performed in
the sequential version, there was no race conditions in these calculations. %explain!!!
To parallelize the calculation of the R matrix, the same tecnhique used in the matrix B 
calculation was used: parallelize the outer loop. In both cases, trying to parallelize
the most inner loop only brought a time loss when comparing to the outer loop parallelization

%...

As for the scheduling, using dynamic brought a better speedup in some cases, but also made
some instances slower. Taking this in consideration, the decision to keep a static schedule
was made. %will this be true?

%...

It is also important to note that, in bigger instances, the parsing creates a considerable 
overhead, making the speedup significantly worse.

%still rough
After the optimizations, the performance for the sequential version fitted within 
expectations. This can be attributed to the numerous optimizations and data structures used.
As for the parallel version, the speedups were quite good taking into account how we decided 
to tackle the parallelization. In the smaller instances the speedup was worse, as expected,
mainly because of the overhead introduced by the parallelization process. Excluding the 
instances where the parsing creates an overhead, the speedups were quite good, getting between X %TODO 
and 3.5.


%ADD PRETTY GRAPHS!
%add tables
\end{document}

