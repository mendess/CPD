\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[cache=false]{minted}
\usepackage[dvipsnames]{xcolor}
\usepackage{a4wide,syntax,listings,appendix,tikz,wrapfig,graphicx,hyperref}
\usepackage[margin=0.8in]{geometry}
\usepackage{pgfplots}
\usepackage{appendix}
\hypersetup{pdftitle={g08report},
pdfauthor={Pedro Mendes},
colorlinks=true,
urlcolor=blue,
linkcolor=black}
%\usetikzlibrary{arrows,positioning,automata,decorations.markings,shadows,shapes,calc}

\begin{document}

\title{Recomender System}
\author{Pedro Mendes (97144), Filipe Lucas (78775), Ricardo Pereira (86506)}
\date{\today}
\maketitle
%\tableofcontents


\section{Serial} %delete or move relevant info to another place
There was no change in the serial version, as it is working as intended. One important 
thing to note is that the MPI version uses the serial version when the number of lines
in matrix A is less than the number of processors, as it wouldn't be possible to perform
the calculations. %is this totally right? 
%maybe change to "as it wouldn't make sense to use MPI for these cases. 

\section{Decomposition}
When discussing how to perform the decomposition, the first idea was to divide the problem 
for the rows in A. This meant that each process would be in charge of calculating the
corresponding elements of matrix L and R. Altough this is a fairly easy
approach, it gave way to some problems and obstacles. To perform this calculations,
it was necessary to have one of the entire L or R matrix in memory, depending if one
of them was transposed. %explain better
Another problem that arose was the impossibility of calculating parts of matrix B in
each node. This meant that, in every iteration, every node would need to send parts of
matrix L and R to the master node so that matrix B could be calculated. Because 
matrix L and R could not fit in memory, the nodes would send chunks of each one as
the master node was calculating matrix B. 
%reword to explicity say that there was a lot of communication

After implementing the aforementioned decomposition and seeing that the amount of
communication was huge problem, a checkerboard decomposition was thought out and 
implemented. To keep decomposition simple, the division was made using squares
instead of rectangles. This allows for simplified operations %talk about MPI_AllReduce
but also brings some limitations: it is only possible to have $N^{2}$, where 
$N = \{2, 3, 4, ...\}$, processors doing work. If a job is run with an invalid number
of processor, the program will calculate the the program will calculate the nearest 
valid number, but smaller than that given as parameter.

%missing: calc L R

Using a checkerboard approach also brings considerable improvements in communication. 
With this approach, each node is capable of independently calculating a chunk of the
B matrix without any communication to the master node. Only after completing all 
iterations the nodes send the chunks of matrix B to the master, where they are
pieced together and the final output is printed.
%talk more about the mechanisms because i forgot some parts :\

\section{Data Structures}
The data structures from the OpenMP version were used, however some new ones were
created to help with the checkerboard decomposition.
\subsection{VMatrix}

\subsection{ABounds}
 

\section{Load Balancing}
%is it worth talking? because even if we split in squares, there could be more non-zero
%elems in some

\section{Performance}
As previously stated, the checkerboard decomposition provides a fairly good advantage 
when comparing to the row decomposition. 
%... compare with serial and maybe openmp?



%talk somewhere about not using openmp 
 

%You must eventually submit the sequential and both parallel versions of your program (please use the
%filenames indicated above), and a table with the times to run the parallel versions on input data that
%will be made available (for 1, 2, 4 and 8 parallel tasks for both OpenMP and MPI, and additionally
%16, 32 and 64 for MPI).
%You must also submit a short report about the results (2 pages) that discusses:
%	the approach used for parallelization
%	what decomposition was used
%	what were the synchronization concerns and why
%	how was load balancing addressed
%	what are the performance results, and are they what you expected


\end{document}

